{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 08 - Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tensorflow libraries\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.applications.xception import decode_predictions\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of ImageDataGenerator with a preprocessing function\n",
    "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create a data generator for traning images\n",
    "train_ds = train_gen.flow_from_directory(\n",
    "    # Path to the directory with the training images\n",
    "    'data/raw/data/train', \n",
    "    # Resize all images to 150x150\n",
    "    target_size=(200,200), \n",
    "    # Number of images to be yielded in each batch\n",
    "    batch_size=20,\n",
    "    class_mode='categorical',\n",
    "    shuffle = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of ImageDataGenerator with a preprocessing function\n",
    "train_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the next batch of images and labels from the generator\n",
    "X, y = next(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 201 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# create a data generator for validation images with prepocessing\n",
    "val_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create a data generator for validation images\n",
    "val_ds = val_gen.flow_from_directory(\n",
    "    'data/raw/data/test', \n",
    "    target_size=(200,200), \n",
    "    batch_size=20,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# Function to define model by adding new dense layer and dropout\n",
    "def make_model(learning_rate=0.01, size_inner=100, droprate=0.5):\n",
    "    base_model = Xception(weights='imagenet',\n",
    "                          include_top=False,\n",
    "                          input_shape=(200,200,3))\n",
    "\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    #########################################\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(200,200,3))\n",
    "    base = base_model(inputs, training=False)\n",
    "    vectors = tf.keras.layers.GlobalAveragePooling2D()(base)\n",
    "    inner = tf.keras.layers.Dense(size_inner, activation='relu')(vectors)\n",
    "    drop = tf.keras.layers.Dropout(droprate)(inner) # add dropout layer\n",
    "    outputs = tf.keras.layers.Dense(2)(drop)  # Change the number of output units to 2\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    #########################################\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "    def make_model(learning_rate=0.002, momentum=0.8):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Input(shape=(200, 200, 3)))\n",
    "        model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(64, activation='relu'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "        optimizer = optimizers.SGD(lr=learning_rate, momentum=momentum)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ xception (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,900</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">202</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ xception (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m20,861,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │       \u001b[38;5;34m204,900\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m202\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,066,582</span> (80.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,066,582\u001b[0m (80.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">205,102</span> (801.18 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m205,102\u001b[0m (801.18 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> (79.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m20,861,480\u001b[0m (79.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zaccaria/.pyenv/versions/3.11.7/envs/class_img/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 634ms/step - accuracy: 0.7545 - loss: 1.0947 - val_accuracy: 0.9403 - val_loss: 0.1390\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 620ms/step - accuracy: 0.9082 - loss: 0.2439 - val_accuracy: 0.9652 - val_loss: 0.1301\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 621ms/step - accuracy: 0.9554 - loss: 0.1359 - val_accuracy: 0.9602 - val_loss: 0.1087\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 620ms/step - accuracy: 0.9481 - loss: 0.1559 - val_accuracy: 0.9652 - val_loss: 0.0898\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 621ms/step - accuracy: 0.9714 - loss: 0.0892 - val_accuracy: 0.9453 - val_loss: 0.1225\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 622ms/step - accuracy: 0.9681 - loss: 0.0703 - val_accuracy: 0.9602 - val_loss: 0.0936\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 620ms/step - accuracy: 0.9723 - loss: 0.0836 - val_accuracy: 0.9602 - val_loss: 0.1252\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 621ms/step - accuracy: 0.9883 - loss: 0.0440 - val_accuracy: 0.9602 - val_loss: 0.1222\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 623ms/step - accuracy: 0.9721 - loss: 0.0895 - val_accuracy: 0.9154 - val_loss: 0.3010\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 623ms/step - accuracy: 0.9506 - loss: 0.1224 - val_accuracy: 0.9303 - val_loss: 0.1227\n",
      "Median Training Accuracy: 0.9637500047683716\n"
     ]
    }
   ],
   "source": [
    "# Train the model and store the history\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=val_ds\n",
    ")\n",
    "\n",
    "# Calculate the median of training accuracy\n",
    "training_accuracies = history.history['accuracy']\n",
    "median_training_accuracy = np.median(training_accuracies)\n",
    "print(f\"Median Training Accuracy: {median_training_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation of Training Loss: 0.188\n"
     ]
    }
   ],
   "source": [
    "# Extract the training losses from the history object\n",
    "training_losses = history.history['loss']\n",
    "\n",
    "# Calculate the standard deviation of the training losses\n",
    "std_training_loss = np.std(training_losses)\n",
    "print(f\"Standard Deviation of Training Loss: {std_training_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of ImageDataGenerator with the specified augmentations\n",
    "train_gen_augmented = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=50,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Create a data generator for training images with augmentations\n",
    "train_ds_augmented = train_gen_augmented.flow_from_directory(\n",
    "    'data/raw/data/train', \n",
    "    target_size=(200,200), \n",
    "    batch_size=20,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# Function to define model by adding new dense layer and dropout\n",
    "def make_model(learning_rate=0.01, size_inner=100, droprate=0.5):\n",
    "    base_model = Xception(weights='imagenet',\n",
    "                          include_top=False,\n",
    "                          input_shape=(200,200,3))\n",
    "\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    #########################################\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(200,200,3))\n",
    "    base = base_model(inputs, training=False)\n",
    "    vectors = tf.keras.layers.GlobalAveragePooling2D()(base)\n",
    "    inner = tf.keras.layers.Dense(size_inner, activation='relu')(vectors)\n",
    "    drop = tf.keras.layers.Dropout(droprate)(inner) # add dropout layer\n",
    "    outputs = tf.keras.layers.Dense(2)(drop)  # Change the number of output units to 2\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    #########################################\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "    def make_model(learning_rate=0.002, momentum=0.8):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Input(shape=(200, 200, 3)))\n",
    "        model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(64, activation='relu'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "        optimizer = optimizers.SGD(lr=learning_rate, momentum=momentum)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 643ms/step - accuracy: 0.9361 - loss: 0.1619 - val_accuracy: 0.9204 - val_loss: 0.1919\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 631ms/step - accuracy: 0.9271 - loss: 0.2068 - val_accuracy: 0.9552 - val_loss: 0.1307\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 630ms/step - accuracy: 0.9375 - loss: 0.1902 - val_accuracy: 0.9254 - val_loss: 0.1492\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 631ms/step - accuracy: 0.9120 - loss: 0.2387 - val_accuracy: 0.9303 - val_loss: 0.1872\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 630ms/step - accuracy: 0.9121 - loss: 0.2084 - val_accuracy: 0.9303 - val_loss: 0.1553\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 632ms/step - accuracy: 0.9428 - loss: 0.2216 - val_accuracy: 0.9403 - val_loss: 0.1756\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 632ms/step - accuracy: 0.9397 - loss: 0.2263 - val_accuracy: 0.9602 - val_loss: 0.1152\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 631ms/step - accuracy: 0.9322 - loss: 0.1541 - val_accuracy: 0.9154 - val_loss: 0.2783\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 632ms/step - accuracy: 0.9427 - loss: 0.1542 - val_accuracy: 0.9154 - val_loss: 0.2145\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 631ms/step - accuracy: 0.9522 - loss: 0.1301 - val_accuracy: 0.9403 - val_loss: 0.1291\n",
      "Median Training Accuracy: 0.9305000007152557\n"
     ]
    }
   ],
   "source": [
    "# Train the model and store the history\n",
    "history = model.fit(\n",
    "    train_ds_augmented,\n",
    "    epochs=10,\n",
    "    validation_data=val_ds\n",
    ")\n",
    "\n",
    "# Calculate the median of training accuracy\n",
    "training_accuracies = history.history['accuracy']\n",
    "average_training_accuracy = np.average(training_accuracies)\n",
    "print(f\"Median Training Accuracy: {average_training_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy for the last 5 epochs: 0.93\n"
     ]
    }
   ],
   "source": [
    "# Extract the validation accuracies from the history object\n",
    "val_accuracies = history.history['val_accuracy']\n",
    "\n",
    "# Calculate the average of the last 5 epochs\n",
    "average_val_accuracy = np.mean(val_accuracies[-5:])\n",
    "print(f\"Average Test Accuracy for the last 5 epochs: {average_val_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Loss for all epochs: 0.17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract the validation losses from the history object\n",
    "val_losses = history.history['val_loss']\n",
    "\n",
    "# Calculate the mean of the validation losses\n",
    "mean_val_loss = np.mean(val_losses)\n",
    "print(f\"Mean Test Loss for all epochs: {mean_val_loss:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class_img",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
